"""
=================================================
ğŸ“Š CAIR4 Prompt-Analyse & Verarbeitung
=================================================

ğŸ“Œ **Beschreibung:**
Damit ein KI-Modell die Texteingabe versteht, muss der Inputtext numerisch umgewandelt werden. 
Dies geschieht mithilfe von **Vektoren**, die es ermÃ¶glichen, das VerhÃ¤ltnis von WÃ¶rtern zueinander zu bestimmen.  
Das KI-System sendet keine Texte an das Modell, sondern **Vektor-ReprÃ¤sentationen** der Eingabe.  
Gleiches gilt fÃ¼r die Antwort: Sie erfolgt als Vektor und wird vom KI-System zurÃ¼ck in Text umgewandelt.  

ğŸ’¡ **Erkunde, wie sich dein Text in Zahlen verwandelt!**  
Gib eine Nachricht ein und analysiere die Verarbeitungsschritte in verschiedenen Visualisierungsformen.

âœ… **Hauptfunktionen:**
- **Schritt-fÃ¼r-Schritt-Analyse der Tokenisierung & Umwandlung in Vektoren**
- **Farbcodierung der Token fÃ¼r bessere VerstÃ¤ndlichkeit**
- **Ãœbersetzungsvergleich mit anderen Sprachen**
- **Interaktive Manipulation einzelner WÃ¶rter und deren Auswirkungen**
- **Speicherung und Nachverfolgung der Transformationen in der Session**
"""

# === ğŸ“¦ Import externer Bibliotheken ===
from pylibs.numpy_lib import numpy as np
from pylibs.pandas_lib import pandas as pd
from pylibs.re_lib import re
from pylibs.random_lib import random
from pylibs.streamlit_lib import streamlit as st
from deep_translator import GoogleTranslator

# === ğŸ›  Import interner Module ===
from utils.core.CAIR4_debug_utils import DebugUtils
from utils.core.CAIR4_session_manager import load_sessions, save_sessions
from utils.core.CAIR4_message_manager import append_message
from utils.core.CAIR4_metrics_manager import update_metrics
from controllers.CAIR4_controller import handle_query


# === ğŸ” Funktion zur Vorverarbeitung von Prompts ===
def preprocess_prompt(prompt):
    """
    Simuliert die Transformation eines Prompts in eine KI-kompatible Form.

    **Schritte der Vorverarbeitung:**
    1ï¸âƒ£ Tokenisierung â†’ Zerlegt den Text in einzelne WÃ¶rter.  
    2ï¸âƒ£ Indizierung â†’ Weist jedem Wort eine ID zu.  
    3ï¸âƒ£ Vektorisierung â†’ Erzeugt fÃ¼r jedes Wort eine numerische Darstellung.  
    4ï¸âƒ£ Positionskodierung â†’ Setzt WÃ¶rter in einen mathematischen Kontext.  
    5ï¸âƒ£ Ãœbersetzung â†’ Vergleicht den deutschen mit dem englischen Text.  
    6ï¸âƒ£ Farbcodierung â†’ Weist Token bestimmte Farben zur Visualisierung zu.  

    Args:
        prompt (str): Vom Benutzer eingegebener Text.

    Returns:
        pandas.DataFrame: Eine Tabelle mit allen Verarbeitungsschritten.
    """
    DebugUtils.debug_print(f"ğŸ”„ Vorverarbeitung gestartet fÃ¼r: {prompt}")

    # 1ï¸âƒ£ Tokenisierung
    tokens = re.findall(r'\b\w+\b', prompt)
    DebugUtils.debug_print(f"âœ… Tokenisierung abgeschlossen: {tokens}")

    # 2ï¸âƒ£ Indizierung
    token_indices = {token: idx for idx, token in enumerate(tokens)}
    DebugUtils.debug_print(f"ğŸ“Š Indizierung: {token_indices}")

    # 3ï¸âƒ£ Dummy-Vektorisierung (One-Hot-Encoding)
    vector_size = len(tokens)
    word_vectors = np.eye(vector_size)

    # 4ï¸âƒ£ Positionskodierung (vereinfachtes Modell)
    position_encoding = {token: idx / vector_size for token, idx in token_indices.items()}
    DebugUtils.debug_print(f"ğŸ“ Positionskodierung: {position_encoding}")

    # 5ï¸âƒ£ Englische Ãœbersetzung
    translated_prompt = GoogleTranslator(source='de', target='en').translate(prompt)
    translated_tokens = re.findall(r'\b\w+\b', translated_prompt)

    # âœ‚ Anpassung der Token-LÃ¤nge (Padding oder Abschneiden)
    if len(translated_tokens) < len(tokens):
        translated_tokens += ["N/A"] * (len(tokens) - len(translated_tokens))
    elif len(translated_tokens) > len(tokens):
        translated_tokens = translated_tokens[:len(tokens)]

    # 6ï¸âƒ£ Farbcodierung (zur besseren Darstellung)
    emoji_palette = [
        # ğŸŸ¦ Quadrate (farbige BlÃ¶cke)
        "ğŸŸ¥", "ğŸŸ§", "ğŸŸ¨", "ğŸŸ©", "ğŸŸ¦", "ğŸŸª", "â¬›", "â¬œ", "ğŸŸ«",

        # â¤ï¸ Emotionen
        "â¤ï¸", "ğŸ§¡", "ğŸ’›", "ğŸ’š", "ğŸ’™", "ğŸ’œ", "ğŸ–¤", "ğŸ¤", "ğŸ¤",

        # âœ¨ Licht & Energie
        "ğŸŒ•", "ğŸŒ‘", "ğŸŒ—", "ğŸŒˆ", "â­", "âœ¨", "ğŸ”¥", "âš¡", "ğŸ’¥",

        # ğŸ”  Symbole
        "ğŸ”", "ğŸ“Œ", "ğŸ§ ", "ğŸ“",

        # â• Bonus-Runde: Stil + Technik
        "ğŸ§¬", "ğŸª", "ğŸ’¡", "ğŸ›°ï¸", "ğŸ–¥ï¸", "ğŸ§®", "ğŸ—‚ï¸", "ğŸ“Š", "ğŸ“š"
    ]

    # ğŸ”€ Mischen & beschneiden auf Anzahl Tokens
    def assign_unique_emojis(tokens):
        unique_count = len(tokens)
        used_palette = emoji_palette.copy()
        random.shuffle(used_palette)
        if unique_count <= len(used_palette):
            return used_palette[:unique_count]
        else:
            # Wenn mehr Tokens als Symbole â†’ auffÃ¼llen mit â¬œ
            return used_palette + ["â¬œ"] * (unique_count - len(used_palette))

    # Ergebnis:
    colors = assign_unique_emojis(tokens)
    # DataFrame fÃ¼r die Analyse erstellen
    df_processing = pd.DataFrame({
        'Token': tokens,
        'Index': [token_indices[token] for token in tokens],
        'Vektor': [list(word_vectors[token_indices[token]]) for token in tokens],
        'Positionskodierung': [position_encoding[token] for token in tokens],
        'Englische Ãœbersetzung': translated_tokens,
        'Symbol': colors
    })

    return df_processing


# === ğŸ— Haupt-Render-Funktion ===
def render_prompt_analyze_view(use_case, context, title, description, session_file, system_message, model_name, settings, collection, sidebar):
    """
    Rendert die Hauptansicht zur **Analyse von Prompts und ihrer Umwandlung in KI-kompatible Daten**.

    Args:
        use_case (str): Der Name des aktuellen Anwendungsfalls.
        context (str): Kontextuelle Informationen.
        title (str): Titel des Views.
        description (str): Beschreibung des Anwendungsfalls.
        session_file (str): Dateipfad zur Speicherung der Sessions.
    """
    DebugUtils.debug_print(f"ğŸ“Œ View gestartet fÃ¼r Use Case: {use_case}")

    # âœ… Sitzungen laden & Initialisieren
    sessions = load_sessions(session_file)
    st.session_state.setdefault("universal_sessions", {})
    st.session_state.universal_sessions[use_case] = sessions

    if "active_use_case" not in st.session_state or st.session_state["active_use_case"] != use_case:
        st.session_state["active_use_case"] = use_case
        st.session_state["current_session"] = {
            "messages": [],
            "metrics": {"total_tokens": 0, "total_costs": 0.0}
        }

    # Hauptansicht
    st.subheader(title)
 
    # ğŸ”¹ Expander fÃ¼r die gewÃ¤hlte Beschreibung
    with st.expander("**Use Case Beschreibung**"):
        st.write(description)

    # ğŸ’¬ **Eingabefeld fÃ¼r Prompts**
    if prompt := st.chat_input("ğŸ’¡ Gib einen Text ein zur Analyse:"):
        DebugUtils.debug_print(f"ğŸ“¥ Eingabe erhalten: {prompt}")
        df = preprocess_prompt(prompt)

        st.dataframe(df)
        st.info("ğŸ“Œ Jeder Schritt zeigt, wie der Text verarbeitet wird, bevor er an das KI-Modell gesendet wird.")

        # ğŸ“œ **Chat speichern**
        append_message(st.session_state["current_session"], "user", prompt)
        append_message(st.session_state["current_session"], "assistant", "Ergebnis wurde generiert.")
        save_sessions(session_file, st.session_state.universal_sessions[use_case])