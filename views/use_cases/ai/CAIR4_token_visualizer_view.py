"""
========================
ðŸ“Š CAIR4 Token Analyzer
========================

Ein Token ist keine Silbe, kein Wort und kein Buchstabe â€“ aber irgendwie *alles davon gleichzeitig*.  
KI-Systeme wie GPT denken nicht in WÃ¶rtern, sondern in **Token** â€“ und jedes Token verbraucht Speicherplatz im GesprÃ¤ch.  

ðŸ“Œ **Beispiel:**  
Das Wort `Internationalisierung` ist **ein** Wort, aber oft **mehrere** Tokens.  
Das Wort `AI` ist sehr kurz â€“ und trotzdem **ein Token**.  
Ein Emoji `ðŸ™‚`? Auch das ist **ein Token**!

---
### âœ… In diesem View:
- Zerlege deinen Text in Tokens und erkenne, was wie gezÃ¤hlt wird.
- Sieh, wie viele Tokens **Input** und **Output** du verbrauchst.
- Verstehe die **Grenzen deines Modells** (z.â€¯B. max. 4096 oder 8192 Tokens).
- SchÃ¤tze ab, wann eine Eingabe zu groÃŸ wird.
"""

# === ðŸ“¦ Import externer Bibliotheken ===
from pylibs.numpy_lib import numpy as np
from pylibs.pandas_lib import pandas as pd
from pylibs.re_lib import re
from pylibs.streamlit_lib import streamlit as st

import tiktoken
import pandas as pd
import streamlit as st

# === Hilfsfunktion zur Tokenanalyse ===
def analyze_tokens(text, model="gpt-3.5-turbo"):
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    token_values = [encoding.decode([t]) for t in tokens]
    return tokens, token_values

# === Heuristik zur semantischen Bewertung von Tokens ===
def semantic_analysis(token_values):
    result = []
    for token in token_values:
        if len(token) <= 2:
            meaning = "âŒ Unwahrscheinlich sinnvoll"
        elif token.lower() in ["der", "die", "und", "ist", "ein"]:
            meaning = "âœ… EigenstÃ¤ndiges Wort"
        elif token[0].isupper() and len(token) > 5:
            meaning = "âš ï¸ MÃ¶glicherweise Teil eines Eigennamens"
        else:
            meaning = "âš ï¸ Eventuell Teilwort"
        result.append(meaning)
    return result

# === Haupt-Renderfunktion ===
def render_token_visualizer_view(use_case, context, title, description, session_file, system_message, model_name, settings, collection, sidebar):
    st.subheader(title)
    with st.expander("**Use Case Beschreibung**"):
        st.write(description)

    # ðŸ“¥ Texteingabe
    prompt = st.text_area("Gib einen beliebigen Text ein:", height=150)

    # Modellwahl (Optional)
    model_choice = st.selectbox("ðŸ”§ Modell wÃ¤hlen (fÃ¼r Tokenizer)", ["gpt-3.5-turbo", "gpt-4"])

    if st.button("Eingabe analysieren"):
        tokens, token_values = analyze_tokens(prompt, model=model_choice)
        semantics = semantic_analysis(token_values)

        # Fortschrittsbalken zur Tokenanzahl
        max_tokens = 4096 if model_choice == "gpt-3.5-turbo" else 8192
        st.progress(min(len(tokens) / max_tokens, 1.0))
        st.info(f"Dein Text besteht aus **{len(tokens)} Token** (max. {max_tokens} fÃ¼r dieses Modell)")

        # Auswahl der Darstellung
        view_mode = st.selectbox("ðŸ§ª Darstellungsvariante wÃ¤hlen:", [
            "ðŸ§¾ Tabellenansicht mit Bewertung",
            "ðŸ”¤ Tokenfolge mit Hervorhebung",
            "ðŸ“‰ Tokenverbrauchs-Simulation"
        ])

        if view_mode == "ðŸ§¾ Tabellenansicht mit Bewertung":
            df = pd.DataFrame({
                "Token-Nr": list(range(1, len(tokens)+1)),
                "Token": token_values,
                "Semantische Bewertung": semantics
            })
            st.dataframe(df, use_container_width=True)

        elif view_mode == "ðŸ”¤ Tokenfolge mit Hervorhebung":
            st.markdown("### ðŸ” Token-Darstellung")
            for i, token in enumerate(token_values):
                badge = semantics[i].split()[0]
                st.markdown(f"{badge} `{token}`", unsafe_allow_html=True)

        elif view_mode == "ðŸ“‰ Tokenverbrauchs-Simulation":
            input_len = len(tokens)
            output_len = st.slider("ðŸ“¤ Erwartete AntwortlÃ¤nge (geschÃ¤tzt in Tokens):", 10, 2048, 200, step=10)
            total = input_len + output_len
            st.markdown(f"ðŸ§® Gesamttokenzahl (Input + Output): **{total}**")
            st.progress(min(total / max_tokens, 1.0))

            if total > max_tokens:
                st.error("ðŸš« Dein Gesamttokenverbrauch Ã¼berschreitet das Limit dieses Modells!")
            else:
                st.success("âœ… Dein Input + erwarteter Output liegen im Rahmen.")
