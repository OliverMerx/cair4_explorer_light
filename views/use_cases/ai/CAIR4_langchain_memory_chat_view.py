"""
=================================================
ğŸ§  CAIR4 Langchain Memory Chat View
=================================================

ğŸ“Œ **Unterschiede zu `Session Memory Chat View`**
Dieses Modul kombiniert **Langchains `ConversationBufferMemory`** mit **CAIR4-Session-Speicherung**.
Dadurch wird sichergestellt, dass:
1ï¸âƒ£ **Langchain Memory** den Chatverlauf fÃ¼r die aktuelle Sitzung speichert.  
2ï¸âƒ£ **CAIR4 Sessions** das gesamte GesprÃ¤ch persistent abspeichert, sodass es auch nach einem Neustart der Anwendung erhalten bleibt.

âœ… **Warum kombinieren?**
- **Langchain `ConversationBufferMemory`** speichert den Verlauf im RAM. Wenn der Server abstÃ¼rzt, ist alles weg.
- **CAIR4 `Session State + JSON-Speicherung`** sichert die Daten in Dateien.
- Diese Kombination erlaubt **eine effiziente, speicherfreundliche Nutzung des Langchain Memory** ohne Token-Overhead.
"""

# === 1ï¸âƒ£ Importiere erforderliche Bibliotheken ===
from pylibs.os_lib import os
from pylibs.streamlit_lib import streamlit as st
from pylibs.json_lib import json
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI

# === 2ï¸âƒ£ Importiere interne CAIR4-Module ===
from utils.core.CAIR4_session_manager import load_sessions, save_sessions
from utils.core.CAIR4_message_manager import append_message
from utils.core.CAIR4_metrics_manager import update_metrics

# === 3ï¸âƒ£ KI-Model initialisieren ===
MODEL_NAME = "gpt-4o"  # Falls du ein anderes Modell mÃ¶chtest, hier Ã¤ndern
llm = ChatOpenAI(model=MODEL_NAME, temperature=0)

# === 4ï¸âƒ£ Conversation Memory & Kettenaufbau ===
def setup_memory_chain():
    """ Erstellt eine Langchain `ConversationBufferMemory` mit dem erwarteten `history` SchlÃ¼ssel. """
    if "chat_memory" not in st.session_state:
        st.session_state.chat_memory = ConversationBufferMemory(memory_key="history")  # ğŸ”¥ **Fix: Jetzt bleibt Memory erhalten**
    
    chain = ConversationChain(llm=llm, memory=st.session_state.chat_memory, verbose=False)
    return chain

# === 5ï¸âƒ£ Hauptansicht fÃ¼r den Chat mit Memory & Session-Persistenz ===
def render_langchain_memory_chat_view(use_case, context, title, description, system_message, session_file, model_name, settings, collection, sidebar):
    """
    Kombiniert Langchains `ConversationBufferMemory` mit CAIR4-Session-Management,
    um sowohl **kurzfristige** als auch **langfristige** ChatverlÃ¤ufe zu ermÃ¶glichen.
    """

    # **ğŸ“‚ Sitzungsladen aus JSON**
    sessions = load_sessions(session_file)
    st.session_state.setdefault("memory_sessions", {})
    st.session_state.memory_sessions[use_case] = sessions

    # **ğŸ§  Langchain Memory initialisieren**
    chain = setup_memory_chain()

    # **ğŸ”„ Falls kein aktiver Use Case existiert oder sich Ã¤ndert, zurÃ¼cksetzen**
    if "active_use_case" not in st.session_state or st.session_state["active_use_case"] != use_case:
        st.session_state["active_use_case"] = use_case
        st.session_state["current_session"] = {
            "messages": [],
            "metrics": {
                "total_tokens": 0,
                "total_costs": 0.0,
                "tokens_used_per_request": [],
                "costs_per_request": [],
                "request_names": [],
            },
        }

    # **ğŸ”„ Nachrichtenverlauf anzeigen (Letzte 5)**
    st.subheader(title)
    with st.expander("**Use Case Beschreibung**"):
        st.write(description)

    for msg in st.session_state["current_session"]["messages"][-5:]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # **ğŸ“ Benutzer-Input**
    prompt = st.chat_input(f"ğŸ’¡ Frage zu {use_case} stellen:")

    if prompt:
        with st.chat_message("user"):
            st.markdown(prompt)
        append_message(st.session_state["current_session"], "user", prompt)

        try:
            with st.chat_message("assistant"):
                with st.spinner("ğŸ”„ Denke nach..."):

                    # **ğŸš€ Langchain Memory verwenden**
                    response = chain.invoke({"input": prompt})

                    # ğŸ” **Key absichern, falls `response` nicht existiert**
                    answer = response.get("response", "âŒ Fehler beim Abrufen der Antwort")

                    st.markdown(answer)
                    append_message(st.session_state["current_session"], "assistant", answer)

                    # **ğŸ“Š Metriken aktualisieren**
                    update_metrics(
                        st.session_state["current_session"]["metrics"],
                        tokens_used=0,  # Langchain speichert keine Token-Daten
                        costs=0.0,  # Kostenberechnung optional
                    )

                    # **ğŸ’¾ Sitzung speichern**
                    st.session_state.memory_sessions[use_case].append(st.session_state["current_session"])
                    save_sessions(session_file, st.session_state.memory_sessions[use_case])

        except Exception as e:
            st.error(f"âš ï¸ Fehler: {e}")