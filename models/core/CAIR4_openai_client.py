"""
===========================================================
OpenAI API Client (generate_answer.py)
===========================================================

Dieses Modul stellt eine Schnittstelle zur **OpenAI API** bereit und erm√∂glicht
die Generierung von Antworten f√ºr Benutzeranfragen mit optionalem Kontext.

üìå Funktionen:
- **generate_answer()** ‚Üí Sendet eine Anfrage an OpenAI mit einstellbaren Parametern.
- **calculate_tokens()** ‚Üí Berechnet die Tokenanzahl einer Eingabe f√ºr verschiedene Modelle.

‚úÖ Warum ist das wichtig?
- Erm√∂glicht eine **kontextbewusste Antwortgenerierung** mit OpenAI-Modellen.
- **Flexibel einstellbar** durch Temperatur, Top-P, Token-Limit und Formatierung.
- **Berechnung der Tokenkosten** f√ºr transparente API-Nutzung.
"""

# === 1Ô∏è‚É£ Import externer Bibliotheken ===
from pylibs.os_lib import os
from pylibs.streamlit_lib import streamlit as st
from pylibs.openai_lib import openai
from pylibs.tiktoken_lib import tiktoken

from models.core.CAIR4_dynaminc_api_keys import get_api_key

OPENAI_API_KEY = get_api_key("openai")

# === 3Ô∏è‚É£ OpenAI-Client initialisieren ===
openai.api_key = OPENAI_API_KEY
client = openai

# === 4Ô∏è‚É£ Sichere Tokenberechnung f√ºr OpenAI-Modelle ===
def calculate_tokens(prompt, model="gpt-4"):
    """ Berechnet die Tokenanzahl basierend auf dem Modell. """
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        print(f"‚ö†Ô∏è Warnung: `{model}` nicht in `tiktoken` verf√ºgbar. Nutze `cl100k_base` als Fallback.")
        encoding = tiktoken.get_encoding("cl100k_base")  # Fallback f√ºr nicht erkannte Modelle

    return len(encoding.encode(prompt))

# === 5Ô∏è‚É£ Hauptfunktion zur Generierung von Antworten ===
def query_openai(
    query,
    context=None,
    system_message="You are a helpful AI assistant.",
    use_context=False,
    temperature=0.7,
    top_p=1.0,
    response_length="Medium",  # üî• Wieder hinzugef√ºgt!
    response_format="Continuous Text",
    max_tokens=None,
    model=None,  # ‚úÖ Nimm den model_name als Parameter entgegen
):
    """
    Erstellt eine Anfrage an die OpenAI API und verarbeitet die Antwort.

    Args:
        query (str): Die Benutzeranfrage.
        context (str, optional): Kontext der Konversation.
        system_message (str): System-Nachricht zur Steuerung der KI.
        use_context (bool): Falls True, wird der Kontext der Anfrage hinzugef√ºgt.
        temperature (float): Steuert die Zuf√§lligkeit der Antwort.
        top_p (float): Alternativer Temperatur-Parameter f√ºr Wahrscheinlichkeitssteuerung.
        response_length (str): L√§nge der Antwort ("Short", "Medium", "Long").
        response_format (str): Format der Antwort ("Bullet Points", "JSON", "Continuous Text").
        max_tokens (int, optional): Maximale Anzahl der Tokens f√ºr die Antwort.
        model (str, optional): Das zu verwendende OpenAI-Modell. ‚úÖ Hinzugef√ºgt!

    Returns:
        str, int, float: Generierte Antwort, Anzahl der verwendeten Tokens, berechnete Kosten.
    """

    max_model_tokens = 8192 if "gpt-4" in model else 4096

    # ‚úÖ Berechne die bereits verwendeten Tokens & vermeide `NoneType`-Fehler
    input_tokens = calculate_tokens(system_message + query, model=model) or 0
    available_tokens = max_model_tokens - input_tokens

    # ‚úÖ Setze `max_tokens` sicher (kein `NoneType`-Fehler mehr)
    if max_tokens is None:
        if response_length == "Short":
            max_tokens = 500
        elif response_length == "Long":
            max_tokens = 4000
        else:  # Standard "Medium"
            max_tokens = 2000

    max_tokens = min(available_tokens, max_tokens)

    # ‚úÖ Passe die System Message basierend auf dem gew√ºnschten Format an
    if response_format == "Bullet Points":
        system_message += " Respond in bullet points."
    elif response_format == "JSON":
        system_message += " Respond in JSON format."
    elif response_format == "Continuous Text":
        system_message += " Provide a detailed but concise explanation."

    if use_context and context:
        system_message += f" Context: {context}"

    # ‚úÖ Nachrichtenstruktur f√ºr OpenAI
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": query.strip()}
    ]

    try:
        completion_params = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens  # Sicher gesetzt, kein `NoneType`
        }

        # ‚úÖ API-Aufruf
        completion = client.chat.completions.create(**completion_params)
        response = completion.choices[0].message.content.strip()

        # ‚úÖ Berechne die tats√§chlichen Token der Antwort & vermeide `NoneType`
        response_tokens = calculate_tokens(response, model=model) or 0
        total_tokens = input_tokens + response_tokens

        print(f"üìä Antwort-Token: {response_tokens}, Gesamt-Token: {total_tokens}")

        # ‚úÖ Kostenberechnung
        cost_per_1k_tokens = 0.03 if "gpt-4" in model else 0.002
        session_cost = (total_tokens / 1000) * cost_per_1k_tokens

        return response, total_tokens, session_cost

    except Exception as e:
        return f"‚ùå Fehler bei der Antwortgenerierung: {e}", 0, 0.0