"""
===========================================================
Mistral API Client (query_mistral.py)
===========================================================

Dieses Modul stellt eine Schnittstelle zur **Mistral API** bereit und erm√∂glicht 
die Verarbeitung von Benutzeranfragen mit kontextabh√§ngigen Prompts.

üìå Funktionen:
- **query_mistral()** ‚Üí Sendet eine Anfrage an Mistral mit Kontext.
- **Automatische Fehlerbehandlung** ‚Üí F√§ngt API-Fehler ab und gibt sinnvolle R√ºckgaben.

‚úÖ Warum ist das wichtig?
- Erm√∂glicht eine direkte Kommunikation mit **Gemini 1.5 Flash** (oder anderen Modellen).
- Unterst√ºtzt kontextbasierte Konversationen.
- Vereinfacht die API-Integration mit Fehlerbehandlung.
"""

# === 1Ô∏è‚É£  Import externer Bibliotheken ===
from pylibs.os_lib import os
from mistralai import Mistral

from models.core.CAIR4_dynaminc_api_keys import get_api_key

#Mistral = mistralai.Mistral()

# Mistral API-Schl√ºssel abrufen
MISTRAL_API_KEY = get_api_key("mistral")

# === 3Ô∏è‚É£  Hauptfunktion zur Kommunikation mit Mistral (erweitert) ===
def query_mistral(
    query,
    context=None,
    system_message="You are a helpful assistant.", 
    response_format="Continuous Text", 
    model_name="mistral-large-latest"
):
    """
    Sendet eine Anfrage an Mistral mit System-Message, Kontext und erwartetem Antwortformat.

    Args:
        query (str): Die neue Benutzeranfrage.
        context (str, optional): Optionaler Kontext f√ºr die Konversation.
        system_message (str): Die System-Message zur Steuerung des Assistentenverhaltens.
        response_format (str): Das gew√ºnschte Antwortformat (z. B. "Bullet Points", "JSON", "Continuous Text").
        model_name (str): Das zu verwendende Modell (Standard: "mistral-large-latest").

    Returns:
        tuple: (Antworttext, Tokenanzahl, Berechnete Kosten)
    """

    client = Mistral(api_key=MISTRAL_API_KEY)

    # ‚úÖ **Response-Format in System-Message integrieren**
    if response_format == "Bullet Points":
        system_message += " Respond in bullet points."
    elif response_format == "JSON":
        system_message += " Respond in JSON format."
    elif response_format == "Continuous Text":
        system_message += " Provide a detailed and continuous explanation."

    # ‚úÖ **Nachrichtenstruktur erstellen**
    messages = [{"role": "system", "content": system_message}]
    
    if context:
        messages.append({"role": "system", "content": f"Context: {context}"})

    messages.append({"role": "user", "content": query})

    try:
        print(f"[DEBUG] Sending messages to Mistral: {messages}")

        chat_response = client.chat.complete(
            model=model_name,
            messages=messages
        )
        
        # ‚úÖ **Sicherstellen, dass die Antwort existiert**
        if not chat_response.choices:
            print("‚ùå [ERROR] Keine Antwort von Mistral erhalten!")
            return "‚ùå Keine Antwort von Mistral erhalten.", 0, 0.0

        # ‚úÖ **Antwortinhalt extrahieren**
        response_content = getattr(chat_response.choices[0].message, "content", None)

        if response_content:
            # Beispielhafte Token- und Kostenberechnung (falls die API keine Infos liefert)
            estimated_tokens = len(response_content.split())  # Einfache Wortz√§hlung
            estimated_cost = estimated_tokens * 0.00001  # Beispiel-Kostenberechnung

            return response_content, estimated_tokens, estimated_cost
        else:
            print("‚ùå [ERROR] Antwort von Mistral ist leer oder ung√ºltig!")
            return "‚ùå Antwort von Mistral ist leer oder ung√ºltig.", 0, 0.0

    except Exception as e:
        print(f"‚ùå Fehler bei der Verarbeitung: {e}")
        return f"‚ùå Fehler bei der Verarbeitung: {e}", 0, 0.0