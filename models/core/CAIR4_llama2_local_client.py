"""
===========================================================
LLaMA 2 API Client (query_llama2.py)
===========================================================

Dieses Modul stellt eine Schnittstelle zur **lokalen LLaMA 2 API** bereit und erm√∂glicht 
die Verarbeitung von Benutzeranfragen mit kontextabh√§ngigen Prompts √ºber **Ollama**.

üìå Funktionen:
- **query_llama2()** ‚Üí Sendet eine Anfrage an das lokal installierte LLaMA 2 Modell.
- **Unterst√ºtzt Kontext** ‚Üí Kann zus√§tzliche Informationen f√ºr genauere Antworten nutzen.
- **Automatische Fehlerbehandlung** ‚Üí Erfasst Fehler und gibt sinnvolle R√ºckgaben.

‚úÖ Warum ist das wichtig?
- Erm√∂glicht eine **lokale** Verarbeitung von KI-Anfragen ohne API-Kosten.
- Unterst√ºtzt Open-Source-Modelle f√ºr Datenschutz und Offline-Verarbeitung.
- Kann in den **Creator, die Main-App und den Digital Twin** integriert werden.
"""

# === 1Ô∏è‚É£  Import externer Bibliotheken ===
from pylibs.streamlit_lib import streamlit as st  # UI-Komponenten f√ºr m√∂gliche Erweiterungen
from pylibs.ollama_lib import ollama # Direktanbindung an die Ollama API f√ºr lokale KI-Modelle

# === 2Ô∏è‚É£  Hauptfunktion zur Kommunikation mit LLaMA 2 ===
def query_llama2(prompt, context="", model_name="llama2", max_length=200):
    """
    Sendet eine Anfrage an das lokal installierte LLaMA 2 Modell √ºber Ollama.

    Args:
        prompt (str): Die Eingabeaufforderung f√ºr das Modell.
        context (str): Optionaler Kontext f√ºr genauere Antworten.
        model_name (str): Das zu verwendende Modell (Standard: "llama2").
        max_length (int): Maximale L√§nge der generierten Antwort.

    Returns:
        str: Die generierte Antwort oder eine Fehlermeldung.
    """

    try:
        # **Erstelle den vollst√§ndigen Prompt mit Kontext**
        full_prompt = f"Kontext: {context}\n\nFrage: {prompt}"

        # **Sende Anfrage an Ollama**
        response = ollama.chat(
            model=model_name,
            messages=[{"role": "user", "content": full_prompt}]
        )

        # **Pr√ºfe die Antwort und extrahiere den generierten Text**
        if "message" in response and "content" in response["message"]:
            return response["message"]["content"]
        else:
            return "‚ùå Fehler: Keine g√ºltige Antwort erhalten."

    except Exception as e:
        print(f"[ERROR] Fehler beim Aufruf von Ollama: {e}")
        return f"‚ùå Fehler: {e}"